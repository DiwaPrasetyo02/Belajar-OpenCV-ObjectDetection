{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OGZrg7JO6AQ"
      },
      "source": [
        "## 8.1 Transfer Learning Faster R-CNN\n",
        "\n",
        "⚠️⚠️⚠️ *Please open this notebook in Google Colab* by click below link ⚠️⚠️⚠️<br><br>\n",
        "<a href=\"https://colab.research.google.com/github/Muhammad-Yunus/Belajar-OpenCV-ObjectDetection/blob/main/Pertemuan%208/8.1%20transfer_learning_faster_rcnn.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><br><br><br>\n",
        "- Click `Connect` button in top right Google Colab notebook,<br>\n",
        "<img src=\"https://github.com/Muhammad-Yunus/Belajar-OpenCV-ObjectDetection/blob/main/Pertemuan%207/resource/cl-connect-gpu.png?raw=1\" width=\"250px\">\n",
        "- If connecting process completed, it will turn to something look like this<br>\n",
        "<img src=\"https://github.com/Muhammad-Yunus/Belajar-OpenCV-ObjectDetection/blob/main/Pertemuan%207/resource/cl-connect-gpu-success.png?raw=1\" width=\"250px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yss_EVvKO6AW"
      },
      "source": [
        "- Check GPU connected into Colab environment is active"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb7xG2MkO6AX"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIZcbHdYYRtS"
      },
      "source": [
        "- Install necesarry package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOSfdK1KYRO-"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics\n",
        "!pip install onnx\n",
        "!pip install onnxruntime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8TXC4uZO6AZ"
      },
      "source": [
        "#### 8.1.1 Download Dataset From Roboflow\n",
        "- Folow instruction in [4.1 dataset_annotation_roboflow.ipynb](https://github/Muhammad-Yunus/Belajar-OpenCV-ObjectDetection/blob/main/Pertemuan%204/84.1%20dataset_annotation_roboflow.ipynb) to prepare `Scissors Dataset` example,\n",
        "- Open `Roboflow` > `Project` > `Versions` menu\n",
        "- Then click `Download Dataset`<br>\n",
        "<img src=\"https://github.com/Muhammad-Yunus/Belajar-OpenCV-ObjectDetection/blob/main/Pertemuan%208/resource/rb-download-dataset.png?raw=1\" width=\"850px\">\n",
        "- Choose `COCO` format and select `Show download code` then click `Continue` <br>\n",
        "<img src=\"https://github.com/Muhammad-Yunus/Belajar-OpenCV-ObjectDetection/blob/main/Pertemuan%208/resource/rb-download-format.png?raw=1\" width=\"350px\">\n",
        "- click `Copy` icon to copy roboflow download code<br>\n",
        "<img src=\"https://github.com/Muhammad-Yunus/Belajar-OpenCV-ObjectDetection/blob/main/Pertemuan%208/resource/rb-copy-download-code.png?raw=1\" width=\"350px\">\n",
        "- Then <font color=\"orange\">replace below code</font> using the copied roboflow download code above,\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DDk3_LnO6AZ"
      },
      "outputs": [],
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"uwcaZm715IIKM3Ej7ix5\")\n",
        "project = rf.workspace(\"learning-k0ow4\").project(\"skissors-detections\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"coco\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCVhLWumO6Aa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "import json\n",
        "import os\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms=None):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # Load JSON annotations\n",
        "        with open(os.path.join(root, \"_annotations.coco.json\")) as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Process images and annotations\n",
        "        self.images = {img['id']: img for img in data['images'] if img['file_name'].endswith('.jpg')}\n",
        "        self.annotations = data['annotations']\n",
        "        self.categories = {cat['id']: cat['name'] for cat in data['categories']}\n",
        "\n",
        "        # Group annotations by image ID for easy lookup\n",
        "        self.img_to_anns = {img_id: [] for img_id in self.images.keys()}\n",
        "        for ann in self.annotations:\n",
        "            self.img_to_anns[ann['image_id']].append(ann)\n",
        "\n",
        "        # Store only images that have annotations\n",
        "        self.imgs = [img_id for img_id in self.img_to_anns if self.img_to_anns[img_id]]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get image and annotations for this index\n",
        "        img_id = self.imgs[idx]\n",
        "        img_info = self.images[img_id]\n",
        "        img_path = os.path.join(self.root, img_info['file_name'])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Get bounding boxes and labels\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for ann in self.img_to_anns[img_id]:\n",
        "            bbox = ann['bbox']\n",
        "            boxes.append([bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]])  # Convert to [x_min, y_min, x_max, y_max]\n",
        "            labels.append(ann['category_id'])\n",
        "\n",
        "        # Convert boxes and labels to tensors\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        target = {\"boxes\": boxes, \"labels\": labels}\n",
        "\n",
        "        # Apply any transforms\n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6apeGxSO6Ab"
      },
      "outputs": [],
      "source": [
        "transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    # Add other augmentations as needed\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPwXwNPDO6Ac"
      },
      "outputs": [],
      "source": [
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = fasterrcnn_resnet50_fpn_v2(weights='DEFAULT')\n",
        "num_classes = 2  # Change this based on your dataset (including background)\n",
        "\n",
        "# Get the number of input features for the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "# Replace the pre-trained head with a new one\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6f2HVGv3O6Ad"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "\n",
        "# Initialize mAP metric\n",
        "map_metric = MeanAveragePrecision(iou_thresholds=[0.5, 0.75, 0.9],  # IoU thresholds\n",
        "#                                  max_detection_thresholds=[10, 100],  # Max detection count (optional)\n",
        "                                  class_metrics=True)  # Separate mAP per class\n",
        "\n",
        "# Assume dataset and DataLoader for training and validation have been set up\n",
        "DATASET_ROOT_PATH = dataset.location\n",
        "dataset_train = CustomDataset(root=f\"{dataset.location}/train\", transforms=transform)\n",
        "dataset_val = CustomDataset(root=f\"{dataset.location}/valid\", transforms=transform)\n",
        "data_loader_train = DataLoader(dataset_train, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "data_loader_val = DataLoader(dataset_val, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "# Move model to device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, validation_loader, device):\n",
        "    model.eval()  # Ensure model is in eval mode\n",
        "\n",
        "    for images, targets in validation_loader:\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        with torch.no_grad():\n",
        "            outputs = model(images)\n",
        "\n",
        "        # Format predictions and targets for torchmetrics\n",
        "        preds = [\n",
        "            {\n",
        "                \"boxes\": output[\"boxes\"].cpu(),\n",
        "                \"scores\": output[\"scores\"].cpu(),\n",
        "                \"labels\": output[\"labels\"].cpu(),\n",
        "            }\n",
        "            for output in outputs\n",
        "        ]\n",
        "\n",
        "        gts = [\n",
        "            {\n",
        "                \"boxes\": target[\"boxes\"].cpu(),\n",
        "                \"labels\": target[\"labels\"].cpu(),\n",
        "            }\n",
        "            for target in targets\n",
        "        ]\n",
        "\n",
        "        # Update mAP metric with predictions and ground truths\n",
        "        map_metric.update(preds, gts)\n",
        "\n",
        "    # Compute mAP at the end of the epoch\n",
        "    map_result = map_metric.compute()\n",
        "    return map_result\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, targets in data_loader_train:\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # Forward pass\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        total_loss += losses.item()\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation step\n",
        "    mAP = evaluate(model, data_loader_val, device)\n",
        "    print(f\"Epoch {epoch+1}, \\tLoss: {total_loss/len(data_loader_train)}, \\tmAP: {mAP['map']:.4f}, \\tmAP-50: {mAP['map_50']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjjlPGyCWBj4"
      },
      "outputs": [],
      "source": [
        "# Export the model to ONNX format\n",
        "MODEL_NAME = \"fasterrcnn_resnet50_fpn_v2_scissors.onnx\"\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    torch.rand(1, 3, 224, 224).to(device),\n",
        "    MODEL_NAME,  # Output path\n",
        "    export_params=True,  # Store trained weights\n",
        "    opset_version=11,    # ONNX opset version\n",
        "    do_constant_folding=True,  # Optimize model by folding constants\n",
        "    input_names=[\"input\"],  # Name of the input layer\n",
        "    output_names=[\"boxes\", \"labels\", \"scores\"],  # Names of output layers\n",
        "    dynamic_axes={\n",
        "        \"input\": {0: \"batch_size\"},  # Variable batch size\n",
        "        \"boxes\": {0: \"num_boxes\"},   # Variable number of boxes\n",
        "        \"labels\": {0: \"num_boxes\"},\n",
        "        \"scores\": {0: \"num_boxes\"}\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "import onnxruntime as ort\n",
        "\n",
        "# Load the ONNX model\n",
        "onnx_model = onnx.load(MODEL_NAME)\n",
        "onnx.checker.check_model(onnx_model)\n",
        "\n",
        "# Run inference with ONNX Runtime\n",
        "ort_session = ort.InferenceSession(MODEL_NAME)\n",
        "\n",
        "# Load your image\n",
        "image_path = \"image2.jpg\"\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "# Define the same transformations used during training\n",
        "transform = T.Compose([\n",
        "    T.Resize((224, 224)),  # Resize to match the model's input size, if needed\n",
        "    T.ToTensor(),          # Convert the image to a tensor\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize using ImageNet stats\n",
        "])\n",
        "\n",
        "# Apply transformations and add batch dimension\n",
        "input_tensor = transform(image).unsqueeze(0)  # Shape: [1, 3, 224, 224]\n",
        "\n",
        "# Convert tensor to numpy format\n",
        "input_image = input_tensor.numpy()\n",
        "\n",
        "# Example inference\n",
        "outputs = ort_session.run(None, {\"input\": input_image})\n",
        "print(\"ONNX model outputs:\", outputs)"
      ],
      "metadata": {
        "id": "0JT6mK-sZXRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Define a function to display the image with detected boxes and labels\n",
        "def display_detections(image_path, boxes, labels, scores, threshold=0.5, class_names=None):\n",
        "    \"\"\"\n",
        "    Displays the image with bounding boxes, labels, and confidence scores.\n",
        "\n",
        "    Parameters:\n",
        "    - image_path: Path to the input image.\n",
        "    - boxes: Bounding boxes from the model output.\n",
        "    - labels: Class labels from the model output.\n",
        "    - scores: Confidence scores from the model output.\n",
        "    - threshold: Minimum confidence score to display a detection.\n",
        "    - class_names: List of class names corresponding to label indices.\n",
        "    \"\"\"\n",
        "    # Load the image\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = image.resize((224, 224))\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(image)\n",
        "\n",
        "    # Create a plot overlay for bounding boxes\n",
        "    ax = plt.gca()\n",
        "\n",
        "    for box, label, score in zip(boxes, labels, scores):\n",
        "        if score >= threshold:  # Only display boxes above the confidence threshold\n",
        "            # Extract box coordinates\n",
        "            x_min, y_min, x_max, y_max = box\n",
        "\n",
        "            # Draw the bounding box\n",
        "            rect = patches.Rectangle(\n",
        "                (x_min, y_min), x_max - x_min, y_max - y_min,\n",
        "                linewidth=2, edgecolor=\"red\", facecolor=\"none\"\n",
        "            )\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "            # Add label and score\n",
        "            label_text = f\"{class_names[label]}: {score:.2f}\" if class_names else f\"Label {label}: {score:.2f}\"\n",
        "            plt.text(\n",
        "                x_min, y_min - 10, label_text,\n",
        "                color=\"red\", fontsize=12, backgroundcolor=\"white\"\n",
        "            )\n",
        "\n",
        "    # Show the plot\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "# Example usage with outputs from the ONNX model\n",
        "image_path = \"image2.jpg\"\n",
        "boxes = outputs[0]  # Bounding boxes\n",
        "labels = outputs[1]  # Class labels\n",
        "scores = outputs[2]  # Confidence scores\n",
        "\n",
        "# Define class names if available, for example: class_names = [\"background\", \"scissors\", \"other\"]\n",
        "class_names = [\"background\", \"scissors\"]  # Modify based on your dataset\n",
        "\n",
        "# Display detections\n",
        "display_detections(image_path, boxes, labels, scores, threshold=0.5, class_names=class_names)\n"
      ],
      "metadata": {
        "id": "s674ymkdbVqg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}