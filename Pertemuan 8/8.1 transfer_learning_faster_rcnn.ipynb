{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OGZrg7JO6AQ"
      },
      "source": [
        "## 8.1 Transfer Learning Faster R-CNN\n",
        "\n",
        "⚠️⚠️⚠️ *Please open this notebook in Google Colab* by click below link ⚠️⚠️⚠️<br><br>\n",
        "<a href=\"https://colab.research.google.com/github/Muhammad-Yunus/Belajar-OpenCV-ObjectDetection/blob/main/Pertemuan%208/8.1%20transfer_learning_faster_rcnn.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><br><br><br>\n",
        "- Click `Connect` button in top right Google Colab notebook,<br>\n",
        "<img src=\"https://github.com/Muhammad-Yunus/Belajar-OpenCV-ObjectDetection/blob/main/Pertemuan%207/resource/cl-connect-gpu.png?raw=1\" width=\"250px\">\n",
        "- If connecting process completed, it will turn to something look like this<br>\n",
        "<img src=\"https://github.com/Muhammad-Yunus/Belajar-OpenCV-ObjectDetection/blob/main/Pertemuan%207/resource/cl-connect-gpu-success.png?raw=1\" width=\"250px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yss_EVvKO6AW"
      },
      "source": [
        "- Check GPU connected into Colab environment is active"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb7xG2MkO6AX",
        "outputId": "2bc8b583-3a57-471e-b74d-dc7f315cab25"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIZcbHdYYRtS"
      },
      "source": [
        "- Install necesarry package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOSfdK1KYRO-"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics\n",
        "!pip install onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8TXC4uZO6AZ"
      },
      "source": [
        "#### 8.1.1 Download Dataset From Roboflow\n",
        "- Folow instruction in [4.1 dataset_annotation_roboflow.ipynb](https://github/Muhammad-Yunus/Belajar-OpenCV-ObjectDetection/blob/main/Pertemuan%204/84.1%20dataset_annotation_roboflow.ipynb) to prepare `Scissors Dataset` example,\n",
        "- Open `Roboflow` > `Project` > `Versions` menu\n",
        "- Then click `Download Dataset`<br>\n",
        "<img src=\"https://github.com/Muhammad-Yunus/Belajar-OpenCV-ObjectDetection/blob/main/Pertemuan%208/resource/rb-download-dataset.png?raw=1\" width=\"850px\">\n",
        "- Choose `COCO` format and select `Show download code` then click `Continue` <br>\n",
        "<img src=\"https://github.com/Muhammad-Yunus/Belajar-OpenCV-ObjectDetection/blob/main/Pertemuan%208/resource/rb-download-format.png?raw=1\" width=\"350px\">\n",
        "- click `Copy` icon to copy roboflow download code<br>\n",
        "<img src=\"https://github.com/Muhammad-Yunus/Belajar-OpenCV-ObjectDetection/blob/main/Pertemuan%208/resource/rb-copy-download-code.png?raw=1\" width=\"350px\">\n",
        "- Then <font color=\"orange\">replace below code</font> using the copied roboflow download code above,\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DDk3_LnO6AZ",
        "outputId": "7fca61f2-d07b-41bd-8e07-21307186d845"
      },
      "outputs": [],
      "source": [
        "# !pip install roboflow\n",
        "\n",
        "# from roboflow import Roboflow\n",
        "# rf = Roboflow(api_key=\"xxxxxxxxxxxxxxxxx\")\n",
        "# project = rf.workspace(\"xxxxxxxxxxxxx\").project(\"xxxxxxxxxxxxxxxxxxx\")\n",
        "# version = project.version(1)\n",
        "# dataset = version.download(\"coco\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NCVhLWumO6Aa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "import json\n",
        "import os\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms=None):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # Load JSON annotations\n",
        "        with open(os.path.join(root, \"_annotations.coco.json\")) as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Process images and annotations\n",
        "        self.images = {img['id']: img for img in data['images'] if img['file_name'].endswith('.jpg')}\n",
        "        self.annotations = data['annotations']\n",
        "        self.categories = {cat['id']: cat['name'] for cat in data['categories']}\n",
        "\n",
        "        # Group annotations by image ID for easy lookup\n",
        "        self.img_to_anns = {img_id: [] for img_id in self.images.keys()}\n",
        "        for ann in self.annotations:\n",
        "            self.img_to_anns[ann['image_id']].append(ann)\n",
        "\n",
        "        # Store only images that have annotations\n",
        "        self.imgs = [img_id for img_id in self.img_to_anns if self.img_to_anns[img_id]]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get image and annotations for this index\n",
        "        img_id = self.imgs[idx]\n",
        "        img_info = self.images[img_id]\n",
        "        img_path = os.path.join(self.root, img_info['file_name'])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Get bounding boxes and labels\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for ann in self.img_to_anns[img_id]:\n",
        "            bbox = ann['bbox']\n",
        "            boxes.append([bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]])  # Convert to [x_min, y_min, x_max, y_max]\n",
        "            labels.append(ann['category_id'])\n",
        "\n",
        "        # Convert boxes and labels to tensors\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        target = {\"boxes\": boxes, \"labels\": labels}\n",
        "\n",
        "        # Apply any transforms\n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "T6apeGxSO6Ab"
      },
      "outputs": [],
      "source": [
        "transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    # Add other augmentations as needed\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "WPwXwNPDO6Ac"
      },
      "outputs": [],
      "source": [
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = fasterrcnn_resnet50_fpn_v2(weights='DEFAULT')\n",
        "num_classes = 2  # Change this based on your dataset (including background)\n",
        "\n",
        "# Get the number of input features for the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "# Replace the pre-trained head with a new one\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f2HVGv3O6Ad",
        "outputId": "ca3bdc6e-e93b-4bbc-8b62-2f7be5723c15"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "\n",
        "# Initialize mAP metric\n",
        "map_metric = MeanAveragePrecision(iou_thresholds=[0.5, 0.75, 0.9],  # IoU thresholds\n",
        "#                                  max_detection_thresholds=[10, 100],  # Max detection count (optional)\n",
        "                                  class_metrics=True)  # Separate mAP per class\n",
        "\n",
        "# Assume dataset and DataLoader for training and validation have been set up\n",
        "DATASET_ROOT_PATH = dataset.location\n",
        "dataset_train = CustomDataset(root=f\"{dataset.location}/train\", transforms=transform)\n",
        "dataset_val = CustomDataset(root=f\"{dataset.location}/valid\", transforms=transform)\n",
        "data_loader_train = DataLoader(dataset_train, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "data_loader_val = DataLoader(dataset_val, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "# Move model to device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, validation_loader, device):\n",
        "    model.eval()  # Ensure model is in eval mode\n",
        "\n",
        "    for images, targets in validation_loader:\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        with torch.no_grad():\n",
        "            outputs = model(images)\n",
        "\n",
        "        # Format predictions and targets for torchmetrics\n",
        "        preds = [\n",
        "            {\n",
        "                \"boxes\": output[\"boxes\"].cpu(),\n",
        "                \"scores\": output[\"scores\"].cpu(),\n",
        "                \"labels\": output[\"labels\"].cpu(),\n",
        "            }\n",
        "            for output in outputs\n",
        "        ]\n",
        "\n",
        "        gts = [\n",
        "            {\n",
        "                \"boxes\": target[\"boxes\"].cpu(),\n",
        "                \"labels\": target[\"labels\"].cpu(),\n",
        "            }\n",
        "            for target in targets\n",
        "        ]\n",
        "\n",
        "        # Update mAP metric with predictions and ground truths\n",
        "        map_metric.update(preds, gts)\n",
        "\n",
        "    # Compute mAP at the end of the epoch\n",
        "    map_result = map_metric.compute()\n",
        "    return map_result\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, targets in data_loader_train:\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # Forward pass\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        total_loss += losses.item()\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation step\n",
        "    mAP = evaluate(model, data_loader_val, device)\n",
        "    print(f\"Epoch {epoch+1}, \\tLoss: {total_loss/len(data_loader_train)}, \\tmAP: {mAP['map']:.4f}, \\tmAP-50: {mAP['map_50']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "id": "PjjlPGyCWBj4",
        "outputId": "b4e7dc63-6d9e-45ce-bb2f-7f7585e0eaa4"
      },
      "outputs": [],
      "source": [
        "# Export the model to ONNX format\n",
        "# Move the dummy input to the same device as the model\n",
        "dummy_input = torch.rand(1, 3, 224, 224).to(device)\n",
        "\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    dummy_input,\n",
        "    \"fasterrcnn_resnet50_fpn_v2_scissors.onnx\",  # Output path\n",
        "    export_params=True,  # Store trained weights\n",
        "    opset_version=11,    # ONNX opset version\n",
        "    do_constant_folding=True,  # Optimize model by folding constants\n",
        "    input_names=[\"input\"],  # Name of the input layer\n",
        "    output_names=[\"boxes\", \"labels\", \"scores\"],  # Names of output layers\n",
        "    dynamic_axes={\n",
        "        \"input\": {0: \"batch_size\"},  # Variable batch size\n",
        "        \"boxes\": {0: \"num_boxes\"},   # Variable number of boxes\n",
        "        \"labels\": {0: \"num_boxes\"},\n",
        "        \"scores\": {0: \"num_boxes\"}\n",
        "    }\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
