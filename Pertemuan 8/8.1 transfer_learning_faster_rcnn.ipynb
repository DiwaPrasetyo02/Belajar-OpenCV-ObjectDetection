{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Transfer Learning Faster R-CNN\n",
    "\n",
    "⚠️⚠️⚠️ *Please open this notebook in Google Colab* by click below link ⚠️⚠️⚠️<br><br>\n",
    "<a href=\"https://colab.research.google.com/github/Muhammad-Yunus/Belajar-OpenCV-ObjectDetection/blob/main/Pertemuan%208/8.1%20transfer_learning_faster_rcnn.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><br><br><br>\n",
    "- Click `Connect` button in top right Google Colab notebook,<br>\n",
    "<img src=\"https://github.com/Muhammad-Yunus/Belajar-OpenCV-ObjectDetection/blob/main/Pertemuan%207/resource/cl-connect-gpu.png?raw=1\" width=\"250px\">\n",
    "- If connecting process completed, it will turn to something look like this<br>\n",
    "<img src=\"https://github.com/Muhammad-Yunus/Belajar-OpenCV-ObjectDetection/blob/main/Pertemuan%207/resource/cl-connect-gpu-success.png?raw=1\" width=\"250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check GPU connected into Colab environment is active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1.1 Download Dataset From Roboflow\n",
    "- Folow instruction in [4.1 dataset_annotation_roboflow.ipynb](https://github/Muhammad-Yunus/Belajar-OpenCV-ObjectDetection/blob/main/Pertemuan%204/84.1%20dataset_annotation_roboflow.ipynb) to prepare `Scissors Dataset` example,\n",
    "- Open `Roboflow` > `Project` > `Versions` menu\n",
    "- Then click `Download Dataset`<br>\n",
    "<img src=\"resource/rb-download-dataset.png\" width=\"850px\">\n",
    "- Choose `COCO` format and select `Show download code` then click `Continue` <br>\n",
    "<img src=\"resource/rb-download-format.png\" width=\"350px\">\n",
    "- click `Copy` icon to copy roboflow download code<br>\n",
    "<img src=\"resource/rb-copy-download-code.png\" width=\"350px\">\n",
    "- Then <font color=\"orange\">replace below code</font> using the copied roboflow download code above,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install roboflow\n",
    "\n",
    "# from roboflow import Roboflow\n",
    "# rf = Roboflow(api_key=\"xxxxxxxxxxxxxxxxxx\")\n",
    "# project = rf.workspace(\"xxxxxxxxxxxxx\").project(\"xxxxxxxxxxxxxxx\")\n",
    "# version = project.version(1)\n",
    "# dataset = version.download(\"coco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # Load all image files and annotations\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"images\"))))\n",
    "        self.annotations = json.load(open(os.path.join(root, \"annotations.json\")))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # Get annotations for this image\n",
    "        ann = self.annotations[idx]\n",
    "        boxes = torch.as_tensor(ann[\"boxes\"], dtype=torch.float32)\n",
    "        labels = torch.as_tensor(ann[\"labels\"], dtype=torch.int64)\n",
    "\n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    # Add other augmentations as needed\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights='DEFAULT')\n",
    "num_classes = 2  # Change this based on your dataset (including background)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torch.nn.Linear(in_features, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision.ops import box_iou\n",
    "\n",
    "# Assume dataset and DataLoader for training and validation have been set up\n",
    "dataset_train = CustomDataset(root=\"path/to/train\", transforms=transform)\n",
    "dataset_val = CustomDataset(root=\"path/to/val\", transforms=transform)\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    iou_thresholds = [0.5, 0.75]  # Set thresholds for mAP calculation\n",
    "    ap_all = []\n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = model(images)\n",
    "            for i, output in enumerate(outputs):\n",
    "                pred_boxes = output['boxes']\n",
    "                pred_scores = output['scores']\n",
    "                gt_boxes = targets[i]['boxes'].to(device)\n",
    "                \n",
    "                # Compute IoU for each prediction and ground truth box\n",
    "                ious = box_iou(pred_boxes, gt_boxes)\n",
    "                matched = (ious > iou_thresholds[0]).sum().item()\n",
    "                \n",
    "                # Calculate AP per threshold\n",
    "                for iou_th in iou_thresholds:\n",
    "                    tp = (ious > iou_th).sum().item()\n",
    "                    fp = len(pred_boxes) - tp\n",
    "                    fn = len(gt_boxes) - tp\n",
    "                    precision = tp / (tp + fp + 1e-6)\n",
    "                    recall = tp / (tp + fn + 1e-6)\n",
    "                    ap_all.append(precision * recall)  # Simplified AP\n",
    "\n",
    "    mAP = sum(ap_all) / len(ap_all)\n",
    "    return mAP\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, targets in data_loader_train:\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        total_loss += losses.item()\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation step\n",
    "    mAP = evaluate(model, data_loader_val, device)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(data_loader_train)}, mAP: {mAP:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
