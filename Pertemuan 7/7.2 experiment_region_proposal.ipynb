{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Experiment Region Proposal\n",
    "\n",
    "⚠️⚠️⚠️ *Please open this notebook in Google Colab* by click below link ⚠️⚠️⚠️<br><br>\n",
    "<a href=\"https://colab.research.google.com/github/Muhammad-Yunus/Belajar-OpenCV-ObjectDetection/blob/main/Pertemuan%207/7.2%20experiment_region_proposal.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><br><br><br>\n",
    "- Click `Connect` button in top right Google Colab notebook,<br>\n",
    "<img src=\"resource/cl-connect-gpu.png\" width=\"250px\">\n",
    "- If connecting process completed, it will turn to something look like this<br>\n",
    "<img src=\"resource/cl-connect-gpu-success.png\" width=\"250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________\n",
    "<br><br><br><br>\n",
    "#### 7.2.1 <font color=\"orange\">Selective Search</font>\n",
    "- Simple implementation Selective Search using OpenCV to perform segmentation based on color similarity\n",
    "- Then producing proposed bounding box as a result countour of segmented image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download an example image from repo\n",
    "import urllib\n",
    "url, filename = (\"https://github.com/Muhammad-Yunus/Belajar-OpenCV-ObjectDetection/raw/main/Pertemuan%207/astronaut.jpg\", \"astronaut.jpg\")\n",
    "try: urllib.URLopener().retrieve(url, filename)\n",
    "except: urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"astronaut.jpg\")\n",
    "\n",
    "# show image using matplot lib\n",
    "def imshow(image):\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define function for color quantization using <font color=\"orange\">K-means clustering</font>,\n",
    "    - Choose Number of <font color=\"orange\">Clusters</font> ($k$),\n",
    "        - For example, if you have a bunch of points and want to split them into 3 groups, you set $k=3$.\n",
    "    - Initialize <font color=\"orange\">Centroids</font>,\n",
    "        - Randomly place $k$ points (called <font color=\"orange\">centroids</font>) in the space where your data points lie. \n",
    "    - Update Centroids,\n",
    "        - For each cluster, calculate the <font color=\"orange\">average</font> of all points assigned to that cluster. \n",
    "        - This <font color=\"orange\">new average</font> becomes the updated <font color=\"orange\">centroid</font> for the cluster.\n",
    "    - <font color=\"orange\">Repeat</font> Until Stable,\n",
    "        - Steps 3 and 4 are repeated until the centroids <font color=\"orange\">no longer change</font> much (or at all) between iterations.\n",
    "        - This means each data point has found its \"best\" cluster.<br><br>\n",
    "        <img src=\"resource/K-Means.gif\" width=\"900px\"><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement K-Means Clustering using OpenCV `cv2.kmeans()`,\n",
    "    ```\n",
    "    retval, bestLabels, centers = cv2.kmeans(data, K, bestLabels, criteria, attempts, flags)\n",
    "    ```\n",
    "    - where :\n",
    "        - `K` Number of clusters to split the set by.\n",
    "        - `bestLabels` Input/output integer array that stores the cluster indices for every sample.\n",
    "        - `criteria` The algorithm termination criteria, can be the maximum number of iterations and/or the desired accuracy. \n",
    "        - `attempts` to specify the number of times the algorithm is executed using different initial labellings. \n",
    "        - `flags` to set random initial centers in each attempt. can take values of,\n",
    "            - cv2.KMEANS_RANDOM_CENTERS     = 0,\n",
    "            - cv2.KMEANS_PP_CENTERS         = 2,\n",
    "            - cv2.KMEANS_USE_INITIAL_LABELS = 1,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_quantization(image, k=5):\n",
    "    # Convert to Lab color space\n",
    "    image_lab = cv2.cvtColor(image, cv2.COLOR_BGR2Lab)\n",
    "\n",
    "    # Reshape the image data to a 2D array of pixels\n",
    "    pixel_values = image_lab.reshape((-1, 3))\n",
    "    pixel_values = np.float32(pixel_values)\n",
    "\n",
    "    # Define criteria and apply KMeans clustering\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 200, 0.2)\n",
    "    _, labels, centers = cv2.kmeans(pixel_values, k, None, criteria, 15, cv2.KMEANS_RANDOM_CENTERS)\n",
    "    \n",
    "    centers = np.uint8(centers)\n",
    "    quantized_image = centers[labels.flatten()]\n",
    "\n",
    "    # Reshape back to the original image dimensions\n",
    "    quantized_image = quantized_image.reshape(image.shape)\n",
    "    return cv2.cvtColor(quantized_image, cv2.COLOR_Lab2BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate segmented image using k=5\n",
    "segmented_image = color_quantization(image, k=5)\n",
    "\n",
    "# show segmented image\n",
    "imshow(segmented_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define function to extract Bounding Box from segmented image\n",
    "    - Convert `segmented_image` to binary image (black & white) using Otsu Thresholding (`cv2.THRESH_OTSU`),\n",
    "    - Find contour on binary image using `cv2.findContours()`,\n",
    "    - Find Bouding Box on detecting contour using `cv2.boundingRect()`\n",
    "    - Filter box size $w*h$ >= $min size$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_boxes(segmented_image, min_size=200):\n",
    "    # Convert to grayscale and apply binary thresholding\n",
    "    gray = cv2.cvtColor(segmented_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Otsu's thresholding\n",
    "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Create bounding boxes for contours larger than min_size\n",
    "    boxes = []\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        if w * h >= min_size:\n",
    "            boxes.append((x, y, w, h))\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = get_bounding_boxes(segmented_image, min_size=500)\n",
    "\n",
    "print(boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Draw bounding box on original image using `cv2.rectangle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define maximun number of box to draw\n",
    "MAX_BOX = 100\n",
    "\n",
    "# draw box in original image\n",
    "image_with_boxes = image.copy()\n",
    "for (x, y, w, h) in boxes[:MAX_BOX]:\n",
    "    cv2.rectangle(image_with_boxes, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "# show image with bounding box\n",
    "imshow(image_with_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______\n",
    "<br><br><br><br>\n",
    "### 7.2.2 <font color=\"orange\">RPN</font> (Region Proposal Network)\n",
    "- <font color=\"orange\">Anchor Generation</font> : Define anchor boxes of multiple scales and aspect ratios for each position in a feature map.\n",
    "- <font color=\"orange\">RPN Network Layers</font> : The RPN typically has a <font color=\"orange\">3x3 convolutional layer</font> to scan over the feature map and two output branches:\n",
    "    - A classification branch (object vs. background) using <font color=\"orange\">1x1 convolutional layer</font>.\n",
    "    - A regression branch (bounding box refinements) using <font color=\"orange\">1x1 convolutional layer</font>.\n",
    "- <font color=\"orange\">Loss Functions</font> : Compute classification and regression loss to train the network.<br>\n",
    "        <img src=\"resource/RPN.png\" width=\"700px\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RPN Implementation in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\D_Drive\\anaconda3\\envs\\BelajarOpenCV\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPNHead(nn.Module):\n",
    "    def __init__(self, in_channels=512, num_anchors=9):\n",
    "        super(RPN, self).__init__()\n",
    "        \n",
    "        # 3x3 Conv layer for sliding window over the feature map\n",
    "        self.conv = nn.Conv2d(in_channels, 512, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Classification layer: predict if each anchor is an object or not\n",
    "        # Output 2 scores per anchor: object or background\n",
    "        self.cls_score = nn.Conv2d(512, num_anchors * 2, kernel_size=1)\n",
    "        \n",
    "        # Regression layer: predict 4 adjustments for each anchor's bbox (dx, dy, dw, dh)\n",
    "        self.bbox_pred = nn.Conv2d(512, num_anchors * 4, kernel_size=1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for layer in [self.conv, self.cls_score, self.bbox_pred]:\n",
    "            # initializes the weights of each layer with a normal (Gaussian) distribution \n",
    "            # with a mean of 0 and a standard deviation of 0.01\n",
    "            nn.init.normal_(layer.weight, std=0.01)\n",
    "            nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shared convolutional layer\n",
    "        x = F.relu(self.conv(x))\n",
    "        \n",
    "        # Classification score for each anchor (object vs. background)\n",
    "        cls_score = self.cls_score(x)\n",
    "        \n",
    "        # Bounding box refinement for each anchor\n",
    "        bbox_pred = self.bbox_pred(x)\n",
    "        \n",
    "        # Reshape for output\n",
    "        # cls_score: [batch, num_anchors * 2, H, W] -> [batch, H * W * num_anchors, 2]\n",
    "        cls_score = cls_score.permute(0, 2, 3, 1).contiguous()\n",
    "        cls_score = cls_score.view(cls_score.shape[0], -1, 2)\n",
    "        \n",
    "        # bbox_pred: [batch, num_anchors * 4, H, W] -> [batch, H * W * num_anchors, 4]\n",
    "        bbox_pred = bbox_pred.permute(0, 2, 3, 1).contiguous()\n",
    "        bbox_pred = bbox_pred.view(bbox_pred.shape[0], -1, 4)\n",
    "        \n",
    "        return cls_score, bbox_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">⚠️⚠️⚠️\n",
    ">- On above implementation, the sliding window concept and anchor-based feature extraction aren't explicitly visible.\n",
    ">- However, the 3x3 convolution inherently works as a sliding window,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load pretrained ResNet-34 from torch.hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained ResNet-34 from torch hub\n",
    "backbone = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=True)\n",
    "\n",
    "# Remove the fully connected layers, keep convolutional layers only\n",
    "backbone = nn.Sequential(*list(backbone.children())[:-2])  # This keeps layers up to the last Conv layer\n",
    "backbone.eval()  # Set backbone to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load Image Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an example image and preprocess it\n",
    "image_path = 'astronaut.jpg'  # Replace with your image path\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image = cv2.resize(image, (224, 224))  # Resize to a suitable input size for ResNet\n",
    "\n",
    "# Convert image to tensor and normalize as required by ResNet\n",
    "input_tensor = torch.tensor(image).float().permute(2, 0, 1).unsqueeze(0)\n",
    "input_tensor = input_tensor / 255.0\n",
    "input_tensor = torchvision.transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Forward pass image tensor to CNN Backbone (Resnet-34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate feature map using ResNet-34\n",
    "with torch.no_grad():  # No need to compute gradients here\n",
    "    feature_map = backbone(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize RPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RPN with input channels matching the ResNet output (512 channels in ResNet-34)\n",
    "rpn_head = RPNHead(in_channels=512, num_anchors=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Forward pass output feature map from backbone network to RPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the feature map through RPN to get region proposals\n",
    "cls_scores, bbox_preds = rpn_head(feature_map)\n",
    "\n",
    "print(f\"Feature map shape: {feature_map.shape}\")\n",
    "print(f\"Class scores shape: {cls_scores.shape}\")\n",
    "print(f\"Bbox predictions shape: {bbox_preds.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create Achor box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class AnchorGenerator:\n",
    "    def __init__(self, sizes=[32, 64, 128], aspect_ratios=[0.5, 1.0, 2.0]):\n",
    "        self.sizes = sizes\n",
    "        self.aspect_ratios = aspect_ratios\n",
    "\n",
    "    def generate_anchors(self, grid_size, stride):\n",
    "        anchors = []\n",
    "        for size in self.sizes:\n",
    "            for aspect_ratio in self.aspect_ratios:\n",
    "                # Compute anchor dimensions\n",
    "                anchor_height = size / math.sqrt(aspect_ratio)\n",
    "                anchor_width = size * math.sqrt(aspect_ratio)\n",
    "\n",
    "                # Generate anchors across the grid\n",
    "                for y in range(grid_size[0]):\n",
    "                    for x in range(grid_size[1]):\n",
    "                        # Convert grid cell coordinates to center coordinates in image space\n",
    "                        center_x = (x + 0.5) * stride\n",
    "                        center_y = (y + 0.5) * stride\n",
    "                        \n",
    "                        # Calculate the (x_min, y_min, x_max, y_max) for each anchor\n",
    "                        x_min = center_x - 0.5 * anchor_width\n",
    "                        y_min = center_y - 0.5 * anchor_height\n",
    "                        x_max = center_x + 0.5 * anchor_width\n",
    "                        y_max = center_y + 0.5 * anchor_height\n",
    "                        \n",
    "                        anchors.append([x_min, y_min, x_max, y_max])\n",
    "\n",
    "        return torch.tensor(anchors)\n",
    "\n",
    "# Example usage\n",
    "anchor_generator = AnchorGenerator(sizes=[32, 64, 128], aspect_ratios=[0.5, 1.0, 2.0])\n",
    "grid_size = (50, 50)  # Example feature map size\n",
    "stride = 16  # Stride of 16 pixels\n",
    "anchors = anchor_generator.generate_anchors(grid_size, stride)\n",
    "print(anchors.shape)  # Should be (num_anchors, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apply Region Proposal Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRegionProposalNetwork(nn.Module):\n",
    "    def __init__(self, anchor_generator, rpn_head, nms_thresh=0.7, num_proposals=1000):\n",
    "        super(SimpleRegionProposalNetwork, self).__init__()\n",
    "        self.anchor_generator = anchor_generator\n",
    "        self.rpn_head = rpn_head\n",
    "        self.nms_thresh = nms_thresh\n",
    "        self.num_proposals = num_proposals\n",
    "\n",
    "    def forward(self, feature_map, image_size):\n",
    "        # 1. Generate anchors\n",
    "        grid_size = feature_map.shape[-2:]  # (height, width)\n",
    "        stride = image_size[0] // grid_size[0]  # Calculate stride based on input and feature map sizes\n",
    "        anchors = self.anchor_generator.generate_anchors(grid_size, stride).to(feature_map.device)\n",
    "        \n",
    "        # 2. Get objectness and bbox deltas from RPN head\n",
    "        objectness, bbox_deltas = self.rpn_head(feature_map)\n",
    "        \n",
    "        # 3. Decode bbox deltas to proposals\n",
    "        proposals = self.apply_deltas_to_anchors(anchors, bbox_deltas)\n",
    "\n",
    "        # 4. Filter proposals using NMS\n",
    "        batch_size = feature_map.shape[0]\n",
    "        final_proposals = []\n",
    "        for i in range(batch_size):\n",
    "            # Select proposals above an objectness threshold (e.g., 0.5) for simplicity\n",
    "            keep = objectness[i] > 0.5\n",
    "            scores = objectness[i][keep]\n",
    "            proposals_for_image = proposals[i][keep]\n",
    "            \n",
    "            # Apply Non-Maximum Suppression (NMS)\n",
    "            keep_indices = self.nms(proposals_for_image, scores, self.nms_thresh)\n",
    "            proposals_after_nms = proposals_for_image[keep_indices]\n",
    "            \n",
    "            # Limit to the top `num_proposals` proposals\n",
    "            final_proposals.append(proposals_after_nms[:self.num_proposals])\n",
    "        \n",
    "        return final_proposals\n",
    "\n",
    "    def apply_deltas_to_anchors(self, anchors, deltas):\n",
    "        anchors = anchors.unsqueeze(0)  # Add batch dimension\n",
    "        widths = anchors[:, :, 2] - anchors[:, :, 0]\n",
    "        heights = anchors[:, :, 3] - anchors[:, :, 1]\n",
    "        ctr_x = anchors[:, :, 0] + 0.5 * widths\n",
    "        ctr_y = anchors[:, :, 1] + 0.5 * heights\n",
    "\n",
    "        dx = deltas[..., 0]\n",
    "        dy = deltas[..., 1]\n",
    "        dw = deltas[..., 2]\n",
    "        dh = deltas[..., 3]\n",
    "\n",
    "        pred_ctr_x = ctr_x + dx * widths\n",
    "        pred_ctr_y = ctr_y + dy * heights\n",
    "        pred_w = torch.exp(dw) * widths\n",
    "        pred_h = torch.exp(dh) * heights\n",
    "\n",
    "        pred_boxes = torch.zeros_like(deltas)\n",
    "        pred_boxes[..., 0] = pred_ctr_x - 0.5 * pred_w\n",
    "        pred_boxes[..., 1] = pred_ctr_y - 0.5 * pred_h\n",
    "        pred_boxes[..., 2] = pred_ctr_x + 0.5 * pred_w\n",
    "        pred_boxes[..., 3] = pred_ctr_y + 0.5 * pred_h\n",
    "\n",
    "        return pred_boxes\n",
    "\n",
    "    def nms(self, boxes, scores, iou_threshold):\n",
    "        keep = []\n",
    "        indices = scores.argsort(descending=True)\n",
    "        while indices.numel() > 0:\n",
    "            current = indices[0]\n",
    "            keep.append(current)\n",
    "            if indices.numel() == 1:\n",
    "                break\n",
    "            ious = self.iou(boxes[current].unsqueeze(0), boxes[indices[1:]])\n",
    "            indices = indices[1:][ious <= iou_threshold]\n",
    "        return torch.tensor(keep, dtype=torch.long, device=boxes.device)\n",
    "\n",
    "    def iou(self, box1, box2):\n",
    "        inter_xmin = torch.max(box1[:, None, 0], box2[:, 0])\n",
    "        inter_ymin = torch.max(box1[:, None, 1], box2[:, 1])\n",
    "        inter_xmax = torch.min(box1[:, None, 2], box2[:, 2])\n",
    "        inter_ymax = torch.min(box1[:, None, 3], box2[:, 3])\n",
    "        \n",
    "        inter_area = (inter_xmax - inter_xmin).clamp(min=0) * (inter_ymax - inter_ymin).clamp(min=0)\n",
    "        box1_area = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])\n",
    "        box2_area = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])\n",
    "        \n",
    "        union_area = box1_area[:, None] + box2_area - inter_area\n",
    "        return inter_area / union_area\n",
    "\n",
    "# Example usage\n",
    "# Initialize with simple anchor generator and RPN head\n",
    "anchor_generator = SimpleAnchorGenerator(sizes=[32, 64, 128], aspect_ratios=[0.5, 1.0, 2.0])\n",
    "rpn_head = SimpleRPNHead(in_channels=512, num_anchors=9)\n",
    "rpn = SimpleRegionProposalNetwork(anchor_generator, rpn_head)\n",
    "\n",
    "# Generate proposals for a dummy feature map\n",
    "feature_map = torch.randn(1, 512, 50, 50)  # (batch_size, in_channels, height, width)\n",
    "image_size = (800, 800)  # Original input image size\n",
    "proposals = rpn(feature_map, image_size)\n",
    "print([p.shape for p in proposals])  # Print the shape of proposals for each image in the batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decoding the Predicted Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection.rpn import RPNHead, RegionProposalNetwork\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torchvision.models.detection.image_list import ImageList \n",
    "\n",
    "# Load an image and preprocess\n",
    "image_path = 'astronaut.jpg'\n",
    "original_image = cv2.imread(image_path)\n",
    "input_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Preprocess image to fit ResNet input size\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),  # Ensure it fits the ResNet input requirements\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "input_tensor = preprocess(input_image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Load ResNet-34 backbone from torch.hub\n",
    "backbone = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=True)\n",
    "backbone = torch.nn.Sequential(*list(backbone.children())[:-2])  # Remove the FC layer\n",
    "\n",
    "# Get the feature map by passing the image through the backbone\n",
    "with torch.no_grad():\n",
    "    feature_map = backbone(input_tensor)  # Shape: [1, C, H, W]\n",
    "\n",
    "# Wrap the feature map in an OrderedDict as expected by torchvision's RPN\n",
    "feature_maps = OrderedDict()\n",
    "feature_maps['0'] = feature_map\n",
    "\n",
    "# Define anchor generator with different scales and aspect ratios\n",
    "anchor_generator = AnchorGenerator(\n",
    "    sizes=((4, 8, 16, 32, 64, 128, 256, 512),),\n",
    "    aspect_ratios=((0.5, 1.0, 2.0),) * len(feature_map.shape[1:])\n",
    ")\n",
    "\n",
    "# Define RPN head (classification and bounding box regression heads)\n",
    "rpn_head = RPNHead(in_channels=feature_map.shape[1], num_anchors=anchor_generator.num_anchors_per_location()[0])\n",
    "\n",
    "# Define Region Proposal Network\n",
    "rpn = RegionProposalNetwork(\n",
    "    anchor_generator=anchor_generator,\n",
    "    head=rpn_head,\n",
    "    fg_iou_thresh=0.8,\n",
    "    bg_iou_thresh=0.3,\n",
    "    batch_size_per_image=256,\n",
    "    positive_fraction=0.5,\n",
    "    pre_nms_top_n={'training': 2000, 'testing': 1000},\n",
    "    post_nms_top_n={'training': 2000, 'testing': 300},\n",
    "    nms_thresh=0.7\n",
    ")\n",
    "\n",
    "image_list = ImageList(input_tensor, [(224, 224)]) \n",
    "\n",
    "# Forward pass through RPN to get proposals\n",
    "rpn.eval()  # Set RPN to evaluation mode\n",
    "with torch.no_grad():\n",
    "    proposals, _ = rpn(image_list, feature_maps)\n",
    "\n",
    "# Extract proposals from output tensor\n",
    "proposals = proposals[0].cpu().numpy()\n",
    "\n",
    "# Calculate scaling factors\n",
    "scale_x = original_image.shape[1] / 224\n",
    "scale_y = original_image.shape[0] / 224\n",
    "\n",
    "# Scale proposals back to original image dimensions\n",
    "rescaled_proposals = []\n",
    "for (x_min, y_min, x_max, y_max) in proposals:\n",
    "    rescaled_proposals.append((\n",
    "        int(x_min * scale_x), int(y_min * scale_y),\n",
    "        int(x_max * scale_x), int(y_max * scale_y)\n",
    "    ))\n",
    "\n",
    "# Draw bounding boxes on the original image\n",
    "image_with_boxes = original_image.copy()\n",
    "for (x_min, y_min, x_max, y_max) in rescaled_proposals[:100]:  # Display the top 100 proposals\n",
    "    cv2.rectangle(image_with_boxes, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "\n",
    "# Display the image with bounding boxes\n",
    "imshow(image_with_boxes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Source\n",
    "- https://www.linkedin.com/pulse/basic-building-blocks-k-means-clustering-algorithms-hemant-thapa-jnide/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
